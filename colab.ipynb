{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef90973fac444eadb26a952d889effff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42cff7f6cbf24c488446d4b149df1d61",
              "IPY_MODEL_9cbf446b7e4448a492a23f73c3fabfd2",
              "IPY_MODEL_45756e717c4f459c8db13596ce392e27"
            ],
            "layout": "IPY_MODEL_bf87ad16795a4fd88da634d68bfa481c"
          }
        },
        "42cff7f6cbf24c488446d4b149df1d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f895a590b4514c8ca71ee0b4747f8220",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb858a99a4f477595ab69ffb3396e4c",
            "value": "model.safetensors: 100%"
          }
        },
        "9cbf446b7e4448a492a23f73c3fabfd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14dd70eaef04d0089314750211a2f61",
            "max": 10241912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e074c16f7d78481288d8ef1d12e0e451",
            "value": 10241912
          }
        },
        "45756e717c4f459c8db13596ce392e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a615b82f8625430f89eb94a2ea38135d",
            "placeholder": "​",
            "style": "IPY_MODEL_d7bc89b640ab4195a5d8c56638192f6b",
            "value": " 10.2M/10.2M [00:00&lt;00:00, 20.3MB/s]"
          }
        },
        "bf87ad16795a4fd88da634d68bfa481c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f895a590b4514c8ca71ee0b4747f8220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb858a99a4f477595ab69ffb3396e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14dd70eaef04d0089314750211a2f61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e074c16f7d78481288d8ef1d12e0e451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a615b82f8625430f89eb94a2ea38135d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7bc89b640ab4195a5d8c56638192f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unified-OneHead Multi-Task Challenge"
      ],
      "metadata": {
        "id": "9naQcB7A-J4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "i-zTu2lYxrCz",
        "outputId": "5a9d137e-3ce0-4fa7-ce85-dba9c5c264fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ef6f083-acb5-4bc2-9cc3-8cc14360bb87\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ef6f083-acb5-4bc2-9cc3-8cc14360bb87\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving hw2.zip to hw2.zip\n"
          ]
        }
      ],
      "source": [
        "# Upload File\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip File\n",
        "import zipfile\n",
        "with zipfile.ZipFile('hw2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./hw2_data')\n"
      ],
      "metadata": {
        "id": "-mkVBjL4zFHl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import timm\n",
        "import time"
      ],
      "metadata": {
        "id": "eZkk-WzfTrG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remapping COCO Dataset\n",
        "COCO_CLASSES = {\n",
        "    1: 0, 2: 1, 3: 2, 4: 3, 5: 4,\n",
        "    6: 5, 7: 6, 8: 7, 9: 8, 10: 9\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WnmYsDHJjLVS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Transforms\n",
        "classify_transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Classification Dataset Loader\n",
        "def get_classification_datasets(train_dir, val_dir, transform):\n",
        "    train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Segmentation Dataset\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, target_size=(224, 224)):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.images = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png'))\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        img = img.resize(self.target_size)\n",
        "        mask = mask.resize(self.target_size, Image.NEAREST)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "# Detection Dataset\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, img_dir, ann_path, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(ann_path)\n",
        "        self.img_ids = list(self.coco.imgs.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "        img = Image.open(os.path.join(self.img_dir, path)).convert('RGB')\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            label = COCO_CLASSES.get(ann['category_id'], None)\n",
        "            if label is None:\n",
        "                continue\n",
        "            bbox = ann['bbox']\n",
        "            scale = 224.0 / 512.0\n",
        "            bbox = [b * scale for b in bbox]\n",
        "\n",
        "            boxes.append(bbox)\n",
        "            labels.append(label)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return img, {'boxes': boxes, 'labels': labels}\n",
        "\n"
      ],
      "metadata": {
        "id": "Tm7Tw8gi7Vsi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Dataset Paths\n",
        "train_path = './hw2_data/hw2/imagenette_160/train'\n",
        "val_path = './hw2_data/hw2/imagenette_160/val'\n",
        "\n",
        "seg_image_dir = './hw2_data/hw2/mini_voc_seg/train/images'\n",
        "seg_mask_dir = './hw2_data/hw2/mini_voc_seg/train/masks'\n",
        "\n",
        "det_image_dir = './hw2_data/hw2/mini_coco_det/train'\n",
        "det_ann_path = './hw2_data/hw2/mini_coco_det/train/instances_train.json'\n",
        "\n",
        "# Transforms for segmentation & detection\n",
        "seg_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "det_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def detection_collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for img, target in batch:\n",
        "        images.append(img)\n",
        "        targets.append(target)\n",
        "    return torch.stack(images, 0), targets\n",
        "\n",
        "# Load Datasets\n",
        "train_set_cls, val_set_cls = get_classification_datasets(train_path, val_path, classify_transform)\n",
        "train_loader_cls = DataLoader(train_set_cls, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader_cls = DataLoader(val_set_cls, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "seg_dataset = VOCDataset(seg_image_dir, seg_mask_dir, transform=seg_transform)\n",
        "seg_loader = DataLoader(seg_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "det_dataset = COCODataset(det_image_dir, det_ann_path, transform=det_transform)\n",
        "det_loader = DataLoader(det_dataset, batch_size=16, shuffle=True, collate_fn=detection_collate_fn)\n",
        "\n",
        "# Checking Dataset\n",
        "print(f\"Classification Classes: {train_set_cls.classes}\")\n",
        "print(f\"Segmentation Dataset Size: {len(seg_dataset)}\")\n",
        "print(f\"Detection Dataset Size: {len(det_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaQ7rAm17ayD",
        "outputId": "58a59ade-0753-477e-a003-54604d46b6e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Classification Classes: ['n01440764', 'n02102040', 'n02979186', 'n03000684', 'n03028079', 'n03394916', 'n03417042', 'n03425413', 'n03445777', 'n03888257']\n",
            "Segmentation Dataset Size: 240\n",
            "Detection Dataset Size: 240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "class UnifiedHead(nn.Module):\n",
        "    def __init__(self, in_ch, num_classes=10, seg_classes=21, det_classes=10):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.det_ch = 5 + det_classes\n",
        "        self.seg_ch = seg_classes\n",
        "        self.cls_ch = num_classes\n",
        "        self.out_channels = self.det_ch + self.seg_ch + self.cls_ch\n",
        "        self.output = nn.Conv2d(128, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        out = self.output(x)\n",
        "\n",
        "        det_out = out[:, :self.det_ch, :, :]\n",
        "        seg_out = out[:, self.det_ch:self.det_ch + self.seg_ch, :, :]\n",
        "        cls_map = out[:, self.det_ch + self.seg_ch:, :, :]\n",
        "\n",
        "        # Classification logits by global average pooling spatially\n",
        "        cls_out = cls_map.mean(dim=(2, 3))\n",
        "\n",
        "        return seg_out, det_out, cls_out\n",
        "\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self, num_classes=10, seg_classes=21, det_classes=10):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('mobilenetv3_small_100', pretrained=True, features_only=True)\n",
        "        ch = self.backbone.feature_info.channels()[-1]\n",
        "        self.head = UnifiedHead(in_ch=ch, num_classes=num_classes, seg_classes=seg_classes, det_classes=det_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)[-1]\n",
        "        return self.head(feat)\n"
      ],
      "metadata": {
        "id": "Zp9tQdbqAk3O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Detection\n",
        "def yolo_detection_loss(pred, targets, C=10, lambda_coord=5.0, lambda_noobj=0.5):\n",
        "    B, _, S, _ = pred.shape\n",
        "    pred = pred.permute(0, 2, 3, 1)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(B):\n",
        "        pred_grid = pred[i]\n",
        "        target = targets[i]\n",
        "        boxes = target['boxes']\n",
        "        labels = target['labels']\n",
        "\n",
        "        obj_mask = torch.zeros((S, S), dtype=torch.bool)\n",
        "        coord_loss, conf_loss, cls_loss, noobj_loss = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        for j in range(len(boxes)):\n",
        "            cx, cy, w, h = boxes[j]\n",
        "            cx_idx = int(cx / 224 * S)\n",
        "            cy_idx = int(cy / 224 * S)\n",
        "\n",
        "            if not (0 <= cx_idx < S and 0 <= cy_idx < S):\n",
        "                continue\n",
        "\n",
        "            obj_mask[cy_idx, cx_idx] = True\n",
        "\n",
        "            # Predicted values\n",
        "            cell = pred_grid[cy_idx, cx_idx]\n",
        "            pred_box = cell[:4]\n",
        "            pred_conf = cell[4]\n",
        "            pred_cls = cell[5:]\n",
        "\n",
        "            # Ground truth relative to cell center\n",
        "            cell_cx = ((cx / 224) * S) - cx_idx\n",
        "            cell_cy = ((cy / 224) * S) - cy_idx\n",
        "            gt_box = torch.tensor([cell_cx, cell_cy, w / 224, h / 224], device=pred.device)\n",
        "\n",
        "            # Losses\n",
        "            coord_loss += F.mse_loss(pred_box, gt_box)\n",
        "            conf_loss += F.binary_cross_entropy_with_logits(pred_conf, torch.tensor(1.0).to(pred_conf.device))\n",
        "            cls_loss += F.cross_entropy(pred_cls.unsqueeze(0), labels[j].unsqueeze(0))\n",
        "\n",
        "        # No-object confidence loss\n",
        "        for y in range(S):\n",
        "            for x in range(S):\n",
        "                if obj_mask[y, x]:\n",
        "                    continue\n",
        "                pred_conf = torch.sigmoid(pred_grid[y, x, 4])\n",
        "                noobj_loss += F.binary_cross_entropy(pred_conf, torch.tensor(0.0).to(pred_conf.device))\n",
        "\n",
        "        total_loss += lambda_coord * coord_loss + cls_loss + conf_loss + lambda_noobj * noobj_loss\n",
        "\n",
        "    return total_loss / B\n"
      ],
      "metadata": {
        "id": "tImtgnZSagJy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Elastic Weight Consolidation\n",
        "class EWC:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
        "        self.fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "    def compute_fisher(self, dataloader, task, device):\n",
        "        self.model.eval()\n",
        "        for img, target in dataloader:\n",
        "            img = img.to(device)\n",
        "            self.model.zero_grad()\n",
        "            seg_out, det_out, cls_out = self.model(img)\n",
        "\n",
        "            if task == 'seg':\n",
        "                target = target.to(device)\n",
        "                seg_out = F.interpolate(seg_out, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
        "                loss = F.cross_entropy(seg_out, target, ignore_index=255)\n",
        "            elif task == 'cls':\n",
        "                target = target.to(device)\n",
        "                loss = F.cross_entropy(cls_out, target)\n",
        "            else:\n",
        "                loss = yolo_detection_loss(det_out, target)\n",
        "\n",
        "            loss.backward()\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if p.grad is not None:\n",
        "                    self.fisher[n] += p.grad.data ** 2\n",
        "        for n in self.fisher:\n",
        "            self.fisher[n] /= len(dataloader)\n",
        "\n",
        "    def penalty(self, model):\n",
        "        loss = 0\n",
        "        for n, p in model.named_parameters():\n",
        "            if n in self.fisher:\n",
        "                loss += (self.fisher[n] * (p - self.params[n]) ** 2).sum()\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "oxKA7y7y7l8l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters Count\n",
        "model = UnifiedModel().to('cpu')\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Total parameters:\", count_parameters(model))\n",
        "print(\"Trainable parameters:\", count_trainable_parameters(model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wGCTtPc_zMH",
        "outputId": "80ff858c-a6da-4848-973e-57e8c985c3bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 1744462\n",
            "Trainable parameters: 1744462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Time\n",
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, 512, 512).to('cpu')\n",
        "with torch.no_grad():\n",
        "    for _ in range(10):\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "# Timing\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(30):\n",
        "        _ = model(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "avg_inference_time = (end - start) / 30 * 1000\n",
        "print(f\"Average inference time: {avg_inference_time:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEiSkBRGApNK",
        "outputId": "1e435e5e-a669-4a57-c1e6-cf814606490c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average inference time: 55.09 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Setup\n",
        "def train(model, dataloader, task, optimizer, device, epochs=30, ewc=None, lambda_ewc=1.0 or 2.0):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        preds_cls, labels_cls = [], []\n",
        "        miou_sum = 0\n",
        "        n_samples = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if task == 'cls':\n",
        "                inputs, targets = batch\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                seg_out, det_out, cls_out = model(inputs)\n",
        "                loss = F.cross_entropy(cls_out, targets)\n",
        "                preds_cls.append(cls_out.argmax(dim=1).cpu())\n",
        "                labels_cls.append(targets.cpu())\n",
        "\n",
        "            elif task == 'seg':\n",
        "                inputs, targets = batch\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                seg_out, det_out, cls_out = model(inputs)\n",
        "                seg_out = F.interpolate(seg_out, size=targets.shape[1:], mode='bilinear', align_corners=False)\n",
        "                loss = F.cross_entropy(seg_out, targets, ignore_index=255)\n",
        "                # Calculate mIoU on CPU\n",
        "                miou_sum += compute_miou(seg_out.detach().cpu(), targets.cpu())\n",
        "                n_samples += 1\n",
        "\n",
        "            elif task == 'det':\n",
        "                inputs, targets = batch\n",
        "                inputs = inputs.to(device)\n",
        "                seg_out, det_out, cls_out = model(inputs)\n",
        "                # Placeholder loss, detection target processing needed\n",
        "                loss = yolo_detection_loss(det_out, targets)\n",
        "\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Task must be one of 'cls', 'seg', 'det'\")\n",
        "\n",
        "            if ewc is not None:\n",
        "                ewc_loss = ewc.penalty(model)\n",
        "                loss += lambda_ewc * ewc_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Task: {task}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if task == 'cls':\n",
        "            acc = accuracy_score(torch.cat(labels_cls), torch.cat(preds_cls))\n",
        "            print(f\"Accuracy: {acc:.4f}\")\n",
        "        elif task == 'seg':\n",
        "            print(f\"mIoU (batch average): {miou_sum / n_samples:.4f}\")\n",
        "\n",
        "def compute_miou(pred_logits, true_masks):\n",
        "    preds = pred_logits.argmax(dim=1)\n",
        "    num_classes = pred_logits.shape[1]\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (preds == cls)\n",
        "        true_cls = (true_masks == cls)\n",
        "        intersection = (pred_cls & true_cls).sum().item()\n",
        "        union = (pred_cls | true_cls).sum().item()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(intersection / union)\n",
        "    if len(ious) == 0:\n",
        "        return 0\n",
        "    return sum(ious) / len(ious)\n"
      ],
      "metadata": {
        "id": "I-NaOBoJ7qyL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline Training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "print(\"\\n=== Training Classification Baseline ===\")\n",
        "train(model, train_loader_cls, task='cls', optimizer=optimizer, device=device, epochs=30)\n",
        "torch.save(model.state_dict(), \"baseline_cls.pth\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "print(\"\\n=== Training Segmentation Baseline ===\")\n",
        "train(model, seg_loader, task='seg', optimizer=optimizer, device=device, epochs=30)\n",
        "torch.save(model.state_dict(), \"baseline_seg.pth\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "print(\"\\n=== Training Detection Baseline ===\")\n",
        "train(model, det_loader, task='det', optimizer=optimizer, device=device, epochs=15)\n",
        "torch.save(model.state_dict(), \"baseline_det.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef90973fac444eadb26a952d889effff",
            "42cff7f6cbf24c488446d4b149df1d61",
            "9cbf446b7e4448a492a23f73c3fabfd2",
            "45756e717c4f459c8db13596ce392e27",
            "bf87ad16795a4fd88da634d68bfa481c",
            "f895a590b4514c8ca71ee0b4747f8220",
            "4cb858a99a4f477595ab69ffb3396e4c",
            "a14dd70eaef04d0089314750211a2f61",
            "e074c16f7d78481288d8ef1d12e0e451",
            "a615b82f8625430f89eb94a2ea38135d",
            "d7bc89b640ab4195a5d8c56638192f6b"
          ]
        },
        "id": "DfEymYbBb2xA",
        "outputId": "5544164b-7990-409b-f910-7398ecae2bb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/10.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef90973fac444eadb26a952d889effff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Classification Baseline ===\n",
            "Epoch 1/30, Task: cls, Loss: 1.8286\n",
            "Accuracy: 0.4417\n",
            "Epoch 2/30, Task: cls, Loss: 1.2939\n",
            "Accuracy: 0.6125\n",
            "Epoch 3/30, Task: cls, Loss: 1.0838\n",
            "Accuracy: 0.6917\n",
            "Epoch 4/30, Task: cls, Loss: 0.8709\n",
            "Accuracy: 0.7875\n",
            "Epoch 5/30, Task: cls, Loss: 0.6360\n",
            "Accuracy: 0.8583\n",
            "Epoch 6/30, Task: cls, Loss: 0.5624\n",
            "Accuracy: 0.8708\n",
            "Epoch 7/30, Task: cls, Loss: 0.5900\n",
            "Accuracy: 0.8625\n",
            "Epoch 8/30, Task: cls, Loss: 0.5283\n",
            "Accuracy: 0.8667\n",
            "Epoch 9/30, Task: cls, Loss: 0.4820\n",
            "Accuracy: 0.8958\n",
            "Epoch 10/30, Task: cls, Loss: 0.4123\n",
            "Accuracy: 0.9042\n",
            "Epoch 11/30, Task: cls, Loss: 0.3950\n",
            "Accuracy: 0.8917\n",
            "Epoch 12/30, Task: cls, Loss: 0.3903\n",
            "Accuracy: 0.8833\n",
            "Epoch 13/30, Task: cls, Loss: 0.3761\n",
            "Accuracy: 0.8958\n",
            "Epoch 14/30, Task: cls, Loss: 0.3995\n",
            "Accuracy: 0.8792\n",
            "Epoch 15/30, Task: cls, Loss: 0.4517\n",
            "Accuracy: 0.8833\n",
            "Epoch 16/30, Task: cls, Loss: 0.2703\n",
            "Accuracy: 0.9375\n",
            "Epoch 17/30, Task: cls, Loss: 0.2333\n",
            "Accuracy: 0.9542\n",
            "Epoch 18/30, Task: cls, Loss: 0.2365\n",
            "Accuracy: 0.9333\n",
            "Epoch 19/30, Task: cls, Loss: 0.2071\n",
            "Accuracy: 0.9458\n",
            "Epoch 20/30, Task: cls, Loss: 0.2198\n",
            "Accuracy: 0.9500\n",
            "Epoch 21/30, Task: cls, Loss: 0.1183\n",
            "Accuracy: 0.9792\n",
            "Epoch 22/30, Task: cls, Loss: 0.1630\n",
            "Accuracy: 0.9625\n",
            "Epoch 23/30, Task: cls, Loss: 0.1382\n",
            "Accuracy: 0.9792\n",
            "Epoch 24/30, Task: cls, Loss: 0.1362\n",
            "Accuracy: 0.9750\n",
            "Epoch 25/30, Task: cls, Loss: 0.1419\n",
            "Accuracy: 0.9583\n",
            "Epoch 26/30, Task: cls, Loss: 0.1428\n",
            "Accuracy: 0.9667\n",
            "Epoch 27/30, Task: cls, Loss: 0.1400\n",
            "Accuracy: 0.9708\n",
            "Epoch 28/30, Task: cls, Loss: 0.1720\n",
            "Accuracy: 0.9583\n",
            "Epoch 29/30, Task: cls, Loss: 0.1182\n",
            "Accuracy: 0.9750\n",
            "Epoch 30/30, Task: cls, Loss: 0.1583\n",
            "Accuracy: 0.9583\n",
            "\n",
            "=== Training Segmentation Baseline ===\n",
            "Epoch 1/30, Task: seg, Loss: 1.8481\n",
            "mIoU (batch average): 0.0902\n",
            "Epoch 2/30, Task: seg, Loss: 1.0590\n",
            "mIoU (batch average): 0.1423\n",
            "Epoch 3/30, Task: seg, Loss: 0.9336\n",
            "mIoU (batch average): 0.1546\n",
            "Epoch 4/30, Task: seg, Loss: 0.8748\n",
            "mIoU (batch average): 0.1612\n",
            "Epoch 5/30, Task: seg, Loss: 0.8233\n",
            "mIoU (batch average): 0.1603\n",
            "Epoch 6/30, Task: seg, Loss: 0.7233\n",
            "mIoU (batch average): 0.1908\n",
            "Epoch 7/30, Task: seg, Loss: 0.6716\n",
            "mIoU (batch average): 0.1997\n",
            "Epoch 8/30, Task: seg, Loss: 0.6025\n",
            "mIoU (batch average): 0.2282\n",
            "Epoch 9/30, Task: seg, Loss: 0.5636\n",
            "mIoU (batch average): 0.2461\n",
            "Epoch 10/30, Task: seg, Loss: 0.5222\n",
            "mIoU (batch average): 0.2745\n",
            "Epoch 11/30, Task: seg, Loss: 0.4767\n",
            "mIoU (batch average): 0.3069\n",
            "Epoch 12/30, Task: seg, Loss: 0.4180\n",
            "mIoU (batch average): 0.3317\n",
            "Epoch 13/30, Task: seg, Loss: 0.3914\n",
            "mIoU (batch average): 0.3634\n",
            "Epoch 14/30, Task: seg, Loss: 0.3551\n",
            "mIoU (batch average): 0.3772\n",
            "Epoch 15/30, Task: seg, Loss: 0.3204\n",
            "mIoU (batch average): 0.4015\n",
            "Epoch 16/30, Task: seg, Loss: 0.3203\n",
            "mIoU (batch average): 0.4102\n",
            "Epoch 17/30, Task: seg, Loss: 0.2812\n",
            "mIoU (batch average): 0.4526\n",
            "Epoch 18/30, Task: seg, Loss: 0.2741\n",
            "mIoU (batch average): 0.4728\n",
            "Epoch 19/30, Task: seg, Loss: 0.2304\n",
            "mIoU (batch average): 0.5038\n",
            "Epoch 20/30, Task: seg, Loss: 0.2161\n",
            "mIoU (batch average): 0.5179\n",
            "Epoch 21/30, Task: seg, Loss: 0.2104\n",
            "mIoU (batch average): 0.5190\n",
            "Epoch 22/30, Task: seg, Loss: 0.1978\n",
            "mIoU (batch average): 0.5141\n",
            "Epoch 23/30, Task: seg, Loss: 0.1905\n",
            "mIoU (batch average): 0.5650\n",
            "Epoch 24/30, Task: seg, Loss: 0.1642\n",
            "mIoU (batch average): 0.5937\n",
            "Epoch 25/30, Task: seg, Loss: 0.1633\n",
            "mIoU (batch average): 0.6059\n",
            "Epoch 26/30, Task: seg, Loss: 0.1545\n",
            "mIoU (batch average): 0.5961\n",
            "Epoch 27/30, Task: seg, Loss: 0.1440\n",
            "mIoU (batch average): 0.6157\n",
            "Epoch 28/30, Task: seg, Loss: 0.1387\n",
            "mIoU (batch average): 0.6262\n",
            "Epoch 29/30, Task: seg, Loss: 0.1320\n",
            "mIoU (batch average): 0.6278\n",
            "Epoch 30/30, Task: seg, Loss: 0.1286\n",
            "mIoU (batch average): 0.6516\n",
            "\n",
            "=== Training Detection Baseline ===\n",
            "Epoch 1/15, Task: det, Loss: 32.2522\n",
            "Epoch 2/15, Task: det, Loss: 25.5389\n",
            "Epoch 3/15, Task: det, Loss: 22.6698\n",
            "Epoch 4/15, Task: det, Loss: 20.4787\n",
            "Epoch 5/15, Task: det, Loss: 18.6407\n",
            "Epoch 6/15, Task: det, Loss: 17.2595\n",
            "Epoch 7/15, Task: det, Loss: 16.0786\n",
            "Epoch 8/15, Task: det, Loss: 15.0469\n",
            "Epoch 9/15, Task: det, Loss: 14.1802\n",
            "Epoch 10/15, Task: det, Loss: 13.3414\n",
            "Epoch 11/15, Task: det, Loss: 12.6206\n",
            "Epoch 12/15, Task: det, Loss: 11.9548\n",
            "Epoch 13/15, Task: det, Loss: 11.3993\n",
            "Epoch 14/15, Task: det, Loss: 10.8062\n",
            "Epoch 15/15, Task: det, Loss: 10.2540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with EWC\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "print(\"\\n=== Training Classification Task (with EWC) ===\")\n",
        "train(model, train_loader_cls, task='cls', optimizer=optimizer, device=device, epochs=30)\n",
        "ewc = EWC(model)\n",
        "ewc.compute_fisher(train_loader_cls, task='cls', device=device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "print(\"\\n=== Training Segmentation Task (with EWC) ===\")\n",
        "train(model, seg_loader, task='seg', optimizer=optimizer, device=device, epochs=30, ewc=ewc, lambda_ewc=3.0)\n",
        "ewc.compute_fisher(seg_loader, task='seg', device=device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "print(\"\\n=== Training Detection Task (with EWC, Placeholder) ===\")\n",
        "train(model, det_loader, task='det', optimizer=optimizer, device=device, epochs=15, ewc=ewc, lambda_ewc=5.0)\n",
        "\n",
        "save_path = \"./unified_model.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRCeylDn7wSi",
        "outputId": "b854db0b-3c68-4c47-ea2e-f8aeb3b94168"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Classification Task (with EWC) ===\n",
            "Epoch 1/30, Task: cls, Loss: 1.8433\n",
            "Accuracy: 0.4125\n",
            "Epoch 2/30, Task: cls, Loss: 1.3190\n",
            "Accuracy: 0.6208\n",
            "Epoch 3/30, Task: cls, Loss: 1.1305\n",
            "Accuracy: 0.6833\n",
            "Epoch 4/30, Task: cls, Loss: 0.8393\n",
            "Accuracy: 0.8125\n",
            "Epoch 5/30, Task: cls, Loss: 0.6797\n",
            "Accuracy: 0.8333\n",
            "Epoch 6/30, Task: cls, Loss: 0.5445\n",
            "Accuracy: 0.8833\n",
            "Epoch 7/30, Task: cls, Loss: 0.5569\n",
            "Accuracy: 0.8792\n",
            "Epoch 8/30, Task: cls, Loss: 0.5548\n",
            "Accuracy: 0.8625\n",
            "Epoch 9/30, Task: cls, Loss: 0.5768\n",
            "Accuracy: 0.8500\n",
            "Epoch 10/30, Task: cls, Loss: 0.6628\n",
            "Accuracy: 0.8167\n",
            "Epoch 11/30, Task: cls, Loss: 0.4077\n",
            "Accuracy: 0.8875\n",
            "Epoch 12/30, Task: cls, Loss: 0.3819\n",
            "Accuracy: 0.9083\n",
            "Epoch 13/30, Task: cls, Loss: 0.3447\n",
            "Accuracy: 0.9083\n",
            "Epoch 14/30, Task: cls, Loss: 0.3139\n",
            "Accuracy: 0.9167\n",
            "Epoch 15/30, Task: cls, Loss: 0.3378\n",
            "Accuracy: 0.9125\n",
            "Epoch 16/30, Task: cls, Loss: 0.2426\n",
            "Accuracy: 0.9625\n",
            "Epoch 17/30, Task: cls, Loss: 0.1962\n",
            "Accuracy: 0.9625\n",
            "Epoch 18/30, Task: cls, Loss: 0.1251\n",
            "Accuracy: 0.9917\n",
            "Epoch 19/30, Task: cls, Loss: 0.1718\n",
            "Accuracy: 0.9667\n",
            "Epoch 20/30, Task: cls, Loss: 0.1439\n",
            "Accuracy: 0.9750\n",
            "Epoch 21/30, Task: cls, Loss: 0.2036\n",
            "Accuracy: 0.9542\n",
            "Epoch 22/30, Task: cls, Loss: 0.1708\n",
            "Accuracy: 0.9667\n",
            "Epoch 23/30, Task: cls, Loss: 0.1737\n",
            "Accuracy: 0.9583\n",
            "Epoch 24/30, Task: cls, Loss: 0.2209\n",
            "Accuracy: 0.9333\n",
            "Epoch 25/30, Task: cls, Loss: 0.1795\n",
            "Accuracy: 0.9625\n",
            "Epoch 26/30, Task: cls, Loss: 0.1991\n",
            "Accuracy: 0.9500\n",
            "Epoch 27/30, Task: cls, Loss: 0.2149\n",
            "Accuracy: 0.9458\n",
            "Epoch 28/30, Task: cls, Loss: 0.1860\n",
            "Accuracy: 0.9542\n",
            "Epoch 29/30, Task: cls, Loss: 0.1609\n",
            "Accuracy: 0.9708\n",
            "Epoch 30/30, Task: cls, Loss: 0.0879\n",
            "Accuracy: 0.9958\n",
            "\n",
            "=== Training Segmentation Task (with EWC) ===\n",
            "Epoch 1/30, Task: seg, Loss: 1.8113\n",
            "mIoU (batch average): 0.0810\n",
            "Epoch 2/30, Task: seg, Loss: 1.0886\n",
            "mIoU (batch average): 0.1273\n",
            "Epoch 3/30, Task: seg, Loss: 0.9519\n",
            "mIoU (batch average): 0.1393\n",
            "Epoch 4/30, Task: seg, Loss: 0.8745\n",
            "mIoU (batch average): 0.1603\n",
            "Epoch 5/30, Task: seg, Loss: 0.8063\n",
            "mIoU (batch average): 0.1710\n",
            "Epoch 6/30, Task: seg, Loss: 0.7622\n",
            "mIoU (batch average): 0.1861\n",
            "Epoch 7/30, Task: seg, Loss: 0.6834\n",
            "mIoU (batch average): 0.2121\n",
            "Epoch 8/30, Task: seg, Loss: 0.6110\n",
            "mIoU (batch average): 0.2317\n",
            "Epoch 9/30, Task: seg, Loss: 0.5659\n",
            "mIoU (batch average): 0.2656\n",
            "Epoch 10/30, Task: seg, Loss: 0.5041\n",
            "mIoU (batch average): 0.2909\n",
            "Epoch 11/30, Task: seg, Loss: 0.4642\n",
            "mIoU (batch average): 0.3209\n",
            "Epoch 12/30, Task: seg, Loss: 0.4171\n",
            "mIoU (batch average): 0.3445\n",
            "Epoch 13/30, Task: seg, Loss: 0.3983\n",
            "mIoU (batch average): 0.3648\n",
            "Epoch 14/30, Task: seg, Loss: 0.3708\n",
            "mIoU (batch average): 0.3742\n",
            "Epoch 15/30, Task: seg, Loss: 0.3183\n",
            "mIoU (batch average): 0.4256\n",
            "Epoch 16/30, Task: seg, Loss: 0.3009\n",
            "mIoU (batch average): 0.4303\n",
            "Epoch 17/30, Task: seg, Loss: 0.2740\n",
            "mIoU (batch average): 0.4868\n",
            "Epoch 18/30, Task: seg, Loss: 0.2534\n",
            "mIoU (batch average): 0.4853\n",
            "Epoch 19/30, Task: seg, Loss: 0.2391\n",
            "mIoU (batch average): 0.4971\n",
            "Epoch 20/30, Task: seg, Loss: 0.2208\n",
            "mIoU (batch average): 0.5231\n",
            "Epoch 21/30, Task: seg, Loss: 0.2022\n",
            "mIoU (batch average): 0.5497\n",
            "Epoch 22/30, Task: seg, Loss: 0.1984\n",
            "mIoU (batch average): 0.5574\n",
            "Epoch 23/30, Task: seg, Loss: 0.2115\n",
            "mIoU (batch average): 0.5206\n",
            "Epoch 24/30, Task: seg, Loss: 0.1910\n",
            "mIoU (batch average): 0.5631\n",
            "Epoch 25/30, Task: seg, Loss: 0.2277\n",
            "mIoU (batch average): 0.5223\n",
            "Epoch 26/30, Task: seg, Loss: 0.2409\n",
            "mIoU (batch average): 0.4960\n",
            "Epoch 27/30, Task: seg, Loss: 0.2139\n",
            "mIoU (batch average): 0.5177\n",
            "Epoch 28/30, Task: seg, Loss: 0.1919\n",
            "mIoU (batch average): 0.5541\n",
            "Epoch 29/30, Task: seg, Loss: 0.1800\n",
            "mIoU (batch average): 0.5839\n",
            "Epoch 30/30, Task: seg, Loss: 0.1637\n",
            "mIoU (batch average): 0.6145\n",
            "\n",
            "=== Training Detection Task (with EWC, Placeholder) ===\n",
            "Epoch 1/15, Task: det, Loss: 32.0985\n",
            "Epoch 2/15, Task: det, Loss: 24.5864\n",
            "Epoch 3/15, Task: det, Loss: 21.4337\n",
            "Epoch 4/15, Task: det, Loss: 19.2026\n",
            "Epoch 5/15, Task: det, Loss: 17.4166\n",
            "Epoch 6/15, Task: det, Loss: 16.0506\n",
            "Epoch 7/15, Task: det, Loss: 14.8908\n",
            "Epoch 8/15, Task: det, Loss: 13.8964\n",
            "Epoch 9/15, Task: det, Loss: 13.0904\n",
            "Epoch 10/15, Task: det, Loss: 12.2729\n",
            "Epoch 11/15, Task: det, Loss: 11.6109\n",
            "Epoch 12/15, Task: det, Loss: 10.9829\n",
            "Epoch 13/15, Task: det, Loss: 10.4052\n",
            "Epoch 14/15, Task: det, Loss: 9.9534\n",
            "Epoch 15/15, Task: det, Loss: 9.3504\n",
            "Model saved to ./unified_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Decoder\n",
        "def decode_predictions(det_out, conf_thresh=0.01, S=7, num_classes=10):\n",
        "    B, _, H, W = det_out.shape\n",
        "    assert H == W == S\n",
        "    pred = det_out.permute(0, 2, 3, 1)  # [B, S, S, 5+C]\n",
        "    decoded = []\n",
        "\n",
        "    for b in range(B):\n",
        "        boxes = []\n",
        "        scores = []\n",
        "        labels = []\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                cell = pred[b, i, j]\n",
        "                cx, cy, w, h = cell[:4]\n",
        "                obj_logit = cell[4]\n",
        "                class_scores = F.softmax(cell[5:], dim=0)\n",
        "\n",
        "                # Apply activations\n",
        "                obj_conf = torch.sigmoid(obj_logit)\n",
        "                cx = torch.sigmoid(cx)\n",
        "                cy = torch.sigmoid(cy)\n",
        "                w = torch.exp(w) if w < 10 else torch.tensor(1.0)  # safety\n",
        "                h = torch.exp(h) if h < 10 else torch.tensor(1.0)\n",
        "\n",
        "                # Compute box center in image space\n",
        "                cx_abs = (j + cx.item()) / S * 224\n",
        "                cy_abs = (i + cy.item()) / S * 224\n",
        "                w_abs = w.item() * 224\n",
        "                h_abs = h.item() * 224\n",
        "\n",
        "                x1 = cx_abs - w_abs / 2\n",
        "                y1 = cy_abs - h_abs / 2\n",
        "                x2 = cx_abs + w_abs / 2\n",
        "                y2 = cy_abs + h_abs / 2\n",
        "\n",
        "                score, cls = class_scores.max(0)\n",
        "                final_score = (score * obj_conf).item()\n",
        "\n",
        "                if final_score > conf_thresh:\n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    scores.append(final_score)\n",
        "                    labels.append(cls.item())\n",
        "\n",
        "        decoded.append({\n",
        "            'boxes': torch.tensor(boxes),\n",
        "            'scores': torch.tensor(scores),\n",
        "            'labels': torch.tensor(labels)\n",
        "        })\n",
        "\n",
        "    return decoded\n"
      ],
      "metadata": {
        "id": "qFfRqXFtsHvP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IoU Computation\n",
        "def compute_iou(box1, box2):\n",
        "    # box: [x1, y1, x2, y2]\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union = box1_area + box2_area - inter_area\n",
        "    return inter_area / union if union > 0 else 0.0\n",
        "def evaluate_map(preds, targets, iou_thresh=0.3):\n",
        "    all_true = 0\n",
        "    all_pred = 0\n",
        "    true_positives = 0\n",
        "\n",
        "    for pred, target in zip(preds, targets):\n",
        "        pred_boxes = pred['boxes']\n",
        "        pred_labels = pred['labels']\n",
        "        gt_boxes = target['boxes']\n",
        "        gt_labels = target['labels']\n",
        "\n",
        "        matched = set()\n",
        "        for i, p_box in enumerate(pred_boxes):\n",
        "            p_label = pred_labels[i]\n",
        "            found_match = False\n",
        "            for j, gt_box in enumerate(gt_boxes):\n",
        "                if j in matched:\n",
        "                    continue\n",
        "                if p_label != gt_labels[j]:\n",
        "                    continue\n",
        "                iou = compute_iou(p_box.tolist(), gt_box.tolist())\n",
        "                if iou >= iou_thresh:\n",
        "                    true_positives += 1\n",
        "                    matched.add(j)\n",
        "                    found_match = True\n",
        "                    break\n",
        "        all_true += len(gt_boxes)\n",
        "        all_pred += len(pred_boxes)\n",
        "\n",
        "    precision = true_positives / all_pred if all_pred > 0 else 0\n",
        "    recall = true_positives / all_true if all_true > 0 else 0\n",
        "    return precision, recall, precision\n"
      ],
      "metadata": {
        "id": "JrJBtmhPsJrs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detection Validation Dataset\n",
        "det_val_image_dir = './hw2_data/hw2/mini_coco_det/val'\n",
        "det_val_ann_path = './hw2_data/hw2/mini_coco_det/val/instances_val.json'\n",
        "\n",
        "det_val_dataset = COCODataset(det_val_image_dir, det_val_ann_path, transform=det_transform)\n",
        "val_loader_det = DataLoader(det_val_dataset, batch_size=8, shuffle=False, collate_fn=detection_collate_fn)\n",
        "\n",
        "# Segmentation Validation Dataset\n",
        "seg_val_image_dir = './hw2_data/hw2/mini_voc_seg/val/images'\n",
        "seg_val_mask_dir = './hw2_data/hw2/mini_voc_seg/val/masks'\n",
        "\n",
        "val_seg_dataset = VOCDataset(seg_val_image_dir, seg_val_mask_dir, transform=seg_transform)\n",
        "val_loader_seg = DataLoader(val_seg_dataset, batch_size=4, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwVl3D-P-J2k",
        "outputId": "2ca0a048-7b03-4218-c057-0be0aeaec7f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Evaluation Function\n",
        "def evaluate_classification(model_path, val_loader, device):\n",
        "    model = UnifiedModel().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            _, _, cls_out = model(x)\n",
        "            preds.append(cls_out.argmax(1).cpu())\n",
        "            labels.append(y.cpu())\n",
        "\n",
        "    acc = accuracy_score(torch.cat(labels), torch.cat(preds))\n",
        "    print(f\"[{os.path.basename(model_path)}] Top-1 Accuracy: {acc:.4f}\")\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "k72u5Es2yOgg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmentation Evaluation Function\n",
        "def evaluate_segmentation(model_path, val_loader, device):\n",
        "    model = UnifiedModel().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    total_miou = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            seg_out, _, _ = model(x)\n",
        "            seg_out = F.interpolate(seg_out, size=y.shape[1:], mode='bilinear', align_corners=False)\n",
        "            total_miou += compute_miou(seg_out.cpu(), y.cpu())\n",
        "            count += 1\n",
        "    miou = total_miou / count\n",
        "    print(f\"[{os.path.basename(model_path)}] mIoU: {miou:.4f}\")\n",
        "    return miou\n"
      ],
      "metadata": {
        "id": "wOJP-wLbyRLZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detection Evaluation Function\n",
        "def evaluate_detection_map(model_path, dataloader, device):\n",
        "    model = UnifiedModel(num_classes=10, seg_classes=21, det_classes=10).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            _, det_out, _ = model(imgs)\n",
        "            preds = decode_predictions(det_out.cpu())\n",
        "            all_preds.extend(preds)\n",
        "            all_targets.extend(targets)\n",
        "\n",
        "    precision, recall, approx_map = evaluate_map(all_preds, all_targets)\n",
        "    print(f\"[{os.path.basename(model_path)}] mAP@0.3 (approx): {approx_map:.4f}\")\n",
        "    return approx_map\n"
      ],
      "metadata": {
        "id": "MUUQMCxzvKOW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "acc_base = evaluate_classification(\"baseline_cls.pth\", val_loader_cls, device)\n",
        "acc_final = evaluate_classification(\"unified_model.pth\", val_loader_cls, device)\n",
        "acc_drop = acc_base - acc_final\n",
        "print(f\"Top-1 drop: {acc_drop:.4f} ({(acc_drop / acc_base) * 100:.2f}%)\")\n",
        "\n",
        "# Segmentation\n",
        "miou_base = evaluate_segmentation(\"baseline_seg.pth\", val_loader_seg, device)\n",
        "miou_final = evaluate_segmentation(\"unified_model.pth\", val_loader_seg, device)\n",
        "miou_drop = miou_base - miou_final\n",
        "print(f\"mIoU drop: {miou_drop:.4f} ({(miou_drop / miou_base) * 100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NrBtL_GyT7D",
        "outputId": "9a69bc86-82e7-454f-b172-d879a1ab5a5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline_cls.pth] Top-1 Accuracy: 0.5833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unified_model.pth] Top-1 Accuracy: 0.1833\n",
            "Top-1 drop: 0.4000 (68.57%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline_seg.pth] mIoU: 0.1004\n",
            "[unified_model.pth] mIoU: 0.0988\n",
            "mIoU drop: 0.0016 (1.60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mAP Calculation\n",
        "baseline_map = evaluate_detection_map(\"baseline_det.pth\", val_loader_det, device)\n",
        "final_map = evaluate_detection_map(\"unified_model.pth\", val_loader_det, device)\n",
        "\n",
        "drop = baseline_map - final_map\n",
        "print(f\"mAP drop: {drop:.4f} ({(drop / baseline_map) * 100:.2f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeCNM2qPvMZZ",
        "outputId": "2a5e3110-195b-41bb-e703-9db2f350bdde"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
            "WARNING:timm.models._builder:Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[baseline_det.pth] mAP@0.3 (approx): 0.0014\n",
            "[unified_model.pth] mAP@0.3 (approx): 0.0017\n",
            "mAP drop: -0.0004 (-26.38%)\n"
          ]
        }
      ]
    }
  ]
}